{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From python list to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 2.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1.0, 2.0, 4.0]\n",
    "a[2], a[1], a[0] # to access the elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing our first tensors\n",
    "Let’s construct our first PyTorch tensor and see what it looks like. It won’t be a particularly  meaningful tensor for now, just three ones in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The essence of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pass a list of lists to the constructor. We can ask the tensor about its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This informs us about the size of the tensor along each dimension. We could also use\n",
    "zeros or ones to initialize the tensor, providing the size as a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points1 = torch.zeros(3, 4)\n",
    "points1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access an individual element in the tensor using two indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 3.],\n",
       "        [9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 3.0],[9.0, 5.0],[1.0, 7.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(1.), tensor(7.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0, 1], points[2, 0], points[2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 7.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing tensors\n",
    "What if we need to obtain a tensor containing all points but the first? That’s easy using\n",
    "range indexing notation, which also applies to standard Python lists. Here’s a\n",
    "reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_list = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[1:6:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t shape is:  torch.Size([3, 5, 5])\n",
      "batch_t shape is:  torch.Size([2, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print('img_t shape is: ', img_t.shape)\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape(batch, channels, rows, columns)\n",
    "print('batch_t shape is: ', batch_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end: they are always in dimension –3, the third from the end. The lazy, unweighted mean can thus be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9475, -0.2048,  0.5480, -1.0728,  0.5295],\n",
       "        [-0.2186, -0.8791, -0.2718, -0.0185,  0.8655],\n",
       "        [-0.2684, -0.2551, -0.0142, -0.3518, -0.2456],\n",
       "        [-0.5062, -0.8001,  0.4826, -0.3146,  0.1570],\n",
       "        [ 0.4514, -0.0372,  0.2800,  0.0095, -0.4125]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2658, -0.8273,  0.5968,  0.5962, -0.5514],\n",
       "         [ 1.0178,  0.2745, -0.6927,  0.0365, -0.1473],\n",
       "         [ 0.2558,  0.0508,  0.2296,  0.8286, -0.0568],\n",
       "         [-0.7043, -0.4299, -0.7622, -0.4874, -0.3776],\n",
       "         [-0.5070, -0.1756,  0.8048, -0.0311,  0.0963]],\n",
       "\n",
       "        [[ 0.5076, -0.4662, -0.1701,  0.5970,  0.5589],\n",
       "         [-0.1031,  0.2303,  0.6683, -0.1983,  0.8043],\n",
       "         [ 1.0771,  0.3059, -0.0527, -1.0095,  0.2989],\n",
       "         [ 0.1800,  0.2465, -0.2021,  0.1847,  0.5674],\n",
       "         [-0.8276, -0.6394, -0.0555, -0.3065, -0.0972]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)\n",
    "batch_gray_naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights are:  tensor([0.2126, 0.7152, 0.0722])\n",
      "tensor([[0.2126],\n",
      "        [0.7152],\n",
      "        [0.0722]])\n"
     ]
    }
   ],
   "source": [
    "print('weights are: ', weights)\n",
    "u_wei = weights.unsqueeze(-1)\n",
    "print(u_wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now the weights is: tensor([[[0.2126]],\n",
      "\n",
      "        [[0.7152]],\n",
      "\n",
      "        [[0.0722]]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueezed weights are\n",
    "unsqueezed_weights = u_wei.unsqueeze_(-1)\n",
    "print('now the weights is:', unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([2, 1, 2, 1, 2, 1, 3, 1, 4, 1, 8])\n",
      "y size;  torch.Size([2, 1, 2, 1, 2, 3, 1, 4, 8])\n",
      "z size;  torch.Size([2, 1, 2, 1, 2, 1, 1, 3, 1, 4, 1, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2, 1, 3, 1, 4, 1, 8)\n",
    "print('x size: ',x.size())\n",
    "y = torch.squeeze(x, 5).squeeze_(-2)\n",
    "print('y size; ', y.size())\n",
    "z = torch.unsqueeze(x, 5).unsqueeze_(-1)\n",
    "print('z size; ', z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsqueeze weights are : torch.Size([3, 1, 1])\n",
      "img weights are : torch.Size([3, 5, 5])\n",
      "batch weights are: tensor([[[[ 1.3410e-01,  4.1412e-02,  1.8624e-01, -2.4739e-01, -8.3834e-03],\n",
      "          [ 2.6002e-02,  1.1006e-01,  2.6604e-02, -1.5176e-01,  1.1341e-01],\n",
      "          [ 5.2646e-03,  8.3194e-02,  9.1663e-03,  5.2155e-02,  3.9999e-01],\n",
      "          [ 5.5885e-02,  2.0322e-01,  7.0203e-02, -4.6984e-02, -4.1005e-01],\n",
      "          [ 1.7054e-01,  4.9045e-02, -2.9808e-01,  1.9825e-01, -1.2489e-01]],\n",
      "\n",
      "         [[-5.7764e-01, -2.0459e-02,  3.5363e-01,  3.3748e-01, -7.9933e-01],\n",
      "          [-6.3657e-01, -4.4475e-01, -5.1425e-01, -4.5333e-01,  6.2717e-01],\n",
      "          [ 4.2966e-01, -2.0583e-01, -2.9130e-01,  1.0615e+00,  3.8507e-01],\n",
      "          [ 6.8312e-01, -3.7356e-01, -7.5505e-01,  9.8167e-01, -8.2856e-02],\n",
      "          [-6.8794e-01, -2.6783e-02, -1.7324e+00,  1.4238e+00, -2.1522e+00]],\n",
      "\n",
      "         [[-4.5049e-02,  8.4379e-02, -5.1102e-02,  3.1131e-02, -1.1852e-02],\n",
      "          [ 1.9891e-01, -3.9633e-02, -9.7409e-02,  8.1302e-02,  4.5220e-02],\n",
      "          [ 1.9301e-02,  2.9437e-02, -1.1874e-01,  7.1562e-02,  8.0219e-02],\n",
      "          [-1.6068e-02, -2.0001e-02,  6.4382e-02,  5.4159e-02,  8.9413e-02],\n",
      "          [-6.6567e-02, -2.9785e-02,  1.2390e-02, -9.5445e-03,  3.2061e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9241e-01, -1.1923e-01, -8.9203e-03,  1.5015e-01, -7.3592e-05],\n",
      "          [-5.4212e-01,  3.2953e-01,  1.6642e-01,  6.9356e-02,  1.4686e-01],\n",
      "          [ 1.1677e-04, -3.0851e-02, -4.9870e-01,  2.4810e-02,  2.0293e-01],\n",
      "          [ 2.4803e-02,  1.3872e-02, -3.0183e-01,  5.9042e-02, -2.4805e-01],\n",
      "          [-6.4960e-03,  1.3815e-01,  1.6640e-01,  1.8815e-01,  1.8227e-01]],\n",
      "\n",
      "         [[-5.0066e-01, -5.5304e-01,  4.2108e-01,  4.2366e-01, -2.3515e-01],\n",
      "          [-1.5401e-01, -8.6034e-01, -3.9332e-01, -5.1129e-01, -2.0767e+00],\n",
      "          [ 1.0850e-01, -1.4833e-01,  2.3785e-01, -1.7973e+00,  1.8245e+00],\n",
      "          [ 6.7472e-01,  8.0928e-01, -2.1390e-01, -6.5202e-01, -1.2857e+00],\n",
      "          [ 9.3133e-01, -7.3583e-01,  4.9366e-01,  2.5744e-01,  2.3087e-01]],\n",
      "\n",
      "         [[-3.1470e-03,  3.3074e-02,  7.0844e-03, -6.0810e-02, -1.8386e-02],\n",
      "          [-5.1670e-02, -6.2190e-02,  2.2253e-02, -3.8278e-02, -8.6203e-02],\n",
      "          [-5.1640e-04, -1.4433e-01,  4.7336e-02, -2.9752e-02, -4.6971e-02],\n",
      "          [-2.6058e-02, -1.1361e-01,  1.0194e-01, -1.4329e-01,  1.0306e-01],\n",
      "          [ 6.7077e-02,  1.3025e-01, -2.6033e-02, -1.2700e-02, -1.4862e-01]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_wieghts = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "print(f'unsqueeze weights are : {unsqueezed_weights.size()}')\n",
    "img_weights = img_t * unsqueezed_weights\n",
    "print(f'img weights are : {img_weights.size()}')\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "print(f'batch weights are: {batch_weights}')\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34946/2371314847.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484772832/work/c10/core/TensorImpl.h:1408.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method align_as returns a tensor with missing \n",
    "dimensions added and existing ones permuted to the right order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions accepting dimension arguments, like sum , also take named dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor element types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41402c8b5e7f8806df4be0126c60543c8d738b82ccbc17a04411a376311c906e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
