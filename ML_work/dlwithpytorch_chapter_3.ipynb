{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From python list to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 2.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1.0, 2.0, 4.0]\n",
    "a[2], a[1], a[0] # to access the elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing our first tensors\n",
    "Let’s construct our first PyTorch tensor and see what it looks like. It won’t be a particularly  meaningful tensor for now, just three ones in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The essence of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pass a list of lists to the constructor. We can ask the tensor about its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This informs us about the size of the tensor along each dimension. We could also use\n",
    "zeros or ones to initialize the tensor, providing the size as a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points1 = torch.zeros(3, 4)\n",
    "points1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access an individual element in the tensor using two indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 3.],\n",
       "        [9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 3.0],[9.0, 5.0],[1.0, 7.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(1.), tensor(7.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0, 1], points[2, 0], points[2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 7.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing tensors\n",
    "What if we need to obtain a tensor containing all points but the first? That’s easy using\n",
    "range indexing notation, which also applies to standard Python lists. Here’s a\n",
    "reminder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_list = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list[1:6:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5.],\n",
       "        [1., 7.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_t shape is:  torch.Size([3, 5, 5])\n",
      "batch_t shape is:  torch.Size([2, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "img_t = torch.randn(3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "print('img_t shape is: ', img_t.shape)\n",
    "batch_t = torch.randn(2, 3, 5, 5) # shape(batch, channels, rows, columns)\n",
    "print('batch_t shape is: ', batch_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end: they are always in dimension –3, the third from the end. The lazy, unweighted mean can thus be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3551, -0.1090,  0.9420,  0.0174, -0.6883],\n",
       "        [-0.5484, -0.8165,  0.6246,  0.1676,  0.4834],\n",
       "        [-0.4048,  0.4419,  0.1145,  0.4918,  0.2158],\n",
       "        [ 1.3552,  0.1854,  0.3461, -0.8790,  0.1873],\n",
       "        [ 1.2316,  0.1752,  0.2710, -0.1936,  0.1626]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "img_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4027,  0.7604, -0.0985, -0.2641,  0.0270],\n",
       "         [-0.1788,  0.2909,  0.2093,  0.3123, -1.7091],\n",
       "         [ 0.8682,  0.0338, -1.1168, -0.6875,  0.1586],\n",
       "         [-0.8242, -0.4347,  0.7018,  0.7854, -0.4348],\n",
       "         [-0.4783,  0.1403, -0.6714, -0.5326, -0.1283]],\n",
       "\n",
       "        [[ 0.5973, -0.5245,  0.0135, -0.8498, -1.2132],\n",
       "         [ 0.2407, -0.1070, -0.2181, -0.4211,  0.4640],\n",
       "         [ 0.2991, -0.4971, -1.2940, -0.4065, -0.9677],\n",
       "         [-0.1969,  0.4618,  0.5780, -0.1594, -0.3836],\n",
       "         [ 0.2466,  0.0106, -0.3765, -0.3065,  0.4799]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_naive = batch_t.mean(-3)\n",
    "batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights are:  tensor([0.2126, 0.7152, 0.0722])\n",
      "tensor([[0.2126],\n",
      "        [0.7152],\n",
      "        [0.0722]])\n"
     ]
    }
   ],
   "source": [
    "print('weights are: ', weights)\n",
    "u_wei = weights.unsqueeze(-1)\n",
    "print(u_wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now the weights is: tensor([[[0.2126]],\n",
      "\n",
      "        [[0.7152]],\n",
      "\n",
      "        [[0.0722]]])\n"
     ]
    }
   ],
   "source": [
    "# unsqueezed weights are\n",
    "unsqueezed_weights = u_wei.unsqueeze_(-1)\n",
    "print('now the weights is:', unsqueezed_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([2, 1, 2, 1, 2, 1, 3, 1, 4, 1, 8])\n",
      "y size;  torch.Size([2, 1, 2, 1, 2, 3, 1, 4, 8])\n",
      "z size;  torch.Size([2, 1, 2, 1, 2, 1, 1, 3, 1, 4, 1, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2, 1, 3, 1, 4, 1, 8)\n",
    "print('x size: ',x.size())\n",
    "y = torch.squeeze(x, 5).squeeze_(-2)\n",
    "print('y size; ', y.size())\n",
    "z = torch.unsqueeze(x, 5).unsqueeze_(-1)\n",
    "print('z size; ', z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsqueeze weights are : torch.Size([3, 1, 1])\n",
      "img weights are : torch.Size([3, 5, 5])\n",
      "batch weights are: tensor([[[[-5.7215e-02,  1.5508e-01, -2.9824e-01,  1.3168e-01,  2.3311e-01],\n",
      "          [ 8.4736e-03, -1.3555e-01, -1.2420e-01, -1.5637e-01, -2.2515e-01],\n",
      "          [-4.0311e-03, -2.9236e-01, -7.8649e-02,  1.0788e-02,  1.3535e-01],\n",
      "          [-2.8815e-01, -1.6792e-02,  4.6567e-02,  3.3994e-01, -2.0604e-01],\n",
      "          [-1.0433e-01,  6.4154e-02,  9.5768e-02, -1.6151e-01, -1.2768e-01]],\n",
      "\n",
      "         [[ 2.1108e-02,  1.9739e-01, -1.8772e-02, -7.6927e-01, -4.6154e-02],\n",
      "          [ 9.4499e-01,  4.5625e-01,  7.9695e-01,  1.2977e+00, -7.2436e-01],\n",
      "          [ 9.6404e-01,  6.8166e-01, -1.4187e+00, -7.8108e-01,  3.9437e-01],\n",
      "          [-7.3646e-01, -2.4148e-01,  5.6921e-01,  8.8831e-01, -5.6055e-01],\n",
      "          [-2.4972e-01,  2.1033e-01, -2.8539e-01,  3.7206e-01, -1.2981e-01]],\n",
      "\n",
      "         [[ 1.0452e-01,  9.2103e-02,  8.1833e-02, -2.4266e-02, -6.8656e-02],\n",
      "          [-1.3700e-01,  6.2993e-02,  7.0672e-03, -1.0271e-02, -2.2060e-01],\n",
      "          [ 9.2093e-02,  3.7788e-02, -7.1973e-02, -7.3734e-02, -5.1418e-02],\n",
      "          [-6.3071e-03, -6.4081e-02,  7.8743e-02, -3.5001e-02,  3.2388e-02],\n",
      "          [-4.2951e-02, -1.2640e-02, -1.4914e-01, -9.8082e-02,  2.8675e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7835e-01, -1.4654e-01,  2.3870e-01,  7.2539e-02, -1.8945e-01],\n",
      "          [ 2.4700e-02,  1.8282e-02,  4.1803e-02, -1.9473e-01,  7.4580e-02],\n",
      "          [-4.5404e-02, -2.5590e-01, -3.5380e-01,  1.7579e-01, -5.6640e-02],\n",
      "          [ 1.2943e-01,  2.5898e-02,  4.2624e-01,  4.8503e-02, -1.1810e-01],\n",
      "          [ 2.3881e-01,  5.3926e-02,  6.2322e-03, -3.1287e-01,  2.6108e-01]],\n",
      "\n",
      "         [[ 9.5681e-02, -8.9043e-01, -1.0309e+00, -6.1144e-01, -7.4802e-01],\n",
      "          [-1.5413e-01, -1.4071e-01,  2.4387e-02, -4.2160e-01,  1.1686e-02],\n",
      "          [ 9.3554e-02, -1.9375e-01, -3.2258e-01, -1.4586e+00, -1.7750e+00],\n",
      "          [ 3.3005e-01,  8.0147e-01,  2.3937e-01, -1.3690e-01, -3.1849e-01],\n",
      "          [ 2.9653e-02,  1.9859e-01, -7.7076e-01,  1.3333e+00, -6.8279e-01]],\n",
      "\n",
      "         [[-4.2742e-02,  2.6051e-02,  2.5924e-02, -1.4697e-01, -1.2292e-01],\n",
      "          [ 5.9315e-02, -1.5174e-02, -6.3903e-02,  1.7479e-02,  7.3986e-02],\n",
      "          [ 7.0769e-02, -1.2066e-03, -1.2755e-01, -5.0428e-04, -1.1183e-02],\n",
      "          [-1.1992e-01,  1.0331e-02, -4.3723e-02, -3.7182e-02, -1.0833e-02],\n",
      "          [-3.0676e-02, -3.6063e-02, -5.8500e-03, -9.4737e-02,  8.4215e-02]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_wieghts = weights.unsqueeze(-1).unsqueeze_(-1)\n",
    "print(f'unsqueeze weights are : {unsqueezed_weights.size()}')\n",
    "img_weights = img_t * unsqueezed_weights\n",
    "print(f'img weights are : {img_weights.size()}')\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "print(f'batch weights are: {batch_weights}')\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5567/2371314847.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484772832/work/c10/core/TensorImpl.h:1408.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method align_as returns a tensor with missing \n",
    "dimensions added and existing ones permuted to the right order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions accepting dimension arguments, like sum , also take named dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to combine dimensions with different names, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5567/1697098842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgray_named\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_named\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mweights_named\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "gray_named = (img_named[..., :3]* weights_named).sum('channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor element types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specifying the numeric type with dtype\n",
    "Here’s a list of the possible values for the ```dtype``` argument:\n",
    "- ```torch.float32``` or ```torch.float```: 32-bit floating-points\n",
    "- ```torch.float64``` or ```torch.doule```: 64-bit, double-precision floating-points\n",
    "- ```torch.float16``` or ```torch.half```: 16-bit, half precision floating-point\n",
    "- ```torch.int8```: signed 8-bit integers\n",
    "- ```torch.uint8```: unsigned 8-bit integers\n",
    "- ```torch.int16``` or ```torch.short```: signed 16-bit integers\n",
    "- ```torch.int32``` or ```torch.int```: signed 32-bit integers\n",
    "- ```torch.int64``` or ```torch.long```: signed 64-bit integers\n",
    "- ```torch.bool```: Boolean\n",
    "\n",
    "However the some basic element of the operation that help:\n",
    "- Tensor can be used ias indexes in other tensors. In this case, Pytorch expects indexing tensor to gave a 64-bit integer data type. Creating a tensor with integers as arguments, such as using ```torch.tensor([2,2])```, will create a 64-bit integer tensor by default.\n",
    "- But we'll speand most of our time dealing with ```float32``` and ```int64```. \n",
    "- Predicates on tensors, such as ```points > 1.0```, produce bool tensors indicating wheather each individual elements satisfies the condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing a tensor’s dtype attribute\n",
    "- To allocate a tensor of the right numeric type, we can specifi=y the proper ```dtype``` as an argument to the constructor. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.ones(10, 2, dtype = torch.double)\n",
    "short_points = torch.tensor([[1, 2],[3, 4]], dtype= torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out about the ```dtype``` for a tensor by accessing the corresponding attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cast the output of a tensor creation function to the right type using the\n",
    "corresponding casting method, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_point = torch.zeros(10, 2).double()\n",
    "short_point = torch.ones(10, 2).short()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make it more convenient to method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_pin = torch.zeros(10, 2).to(torch.double)\n",
    "short_pin = torch.ones(10, 2).to(dtype=torch.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When mixing input types in operations, the inputs are converted to the larger type\n",
    "automatically. Thus, if we want 32-bit computation, we need to make sure all our\n",
    "inputs are (at most) 32-bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype = torch.double)\n",
    "points_shorts = points_64.to(torch.short)\n",
    "points_64 * points_shorts # works from pytorch 1.3 onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tensor API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the vast majority of operations on and between tensors are available in the\n",
    "```torch``` module and can also be called as methods of a tensor object. For instance, the\n",
    "```transpose``` function we encountered earlier can be used from the ```torch``` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as a method of the a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(3, 2)\n",
    "b_t = b.transpose(0, 1)\n",
    "\n",
    "b.shape, b_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0],[5.0, 3.0],[2.0, 1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the ```zero_``` method zeros out all the elements of the input.\n",
    "Any method without the trailing underscore leaves the source tensor unchanged and\n",
    "instead returns a new tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor metadata: Size, offset, and stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Views of another tensor’s storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving tensors to the GPU\n",
    "n addition to ```dtype``` , a PyTorch ```Tensor``` also has the notion of ```device``` , which is whereon the computer the tensor data is placed. Here is how we can create a tensor on the GPU by specifying the corresponding argument to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = 2*points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2 = 2*points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add a constant tot he result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points_gpu + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to move the tensor back to\n",
    "the CPU, we need to provide a ```cpu``` argument to the ```to``` method, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_cpu = points_gpu.to(device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the shorthand methods cpu and cuda instead of the to method to\n",
    "achieve the same goal:\n",
    "```python3\n",
    "points_gpu = points.cuda()\n",
    "points_gpu = points.cuda(0)\n",
    "points_cpu = points_gpu.cpu()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy interoperability\n",
    "To get a NumPy array out of our ```points``` tensor, we just call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.ones(5, 5)\n",
    "points_np = points.numpy() # convert tensor to numpy\n",
    "points_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, we can obtain a PyTorch tensor from a NumPy array this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.from_numpy(points_np)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how we can save our ```points``` tensor to an ourpoints.t file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(points, '/home/mahfuz/Desktop/dlpyt/p1ch3/outpoints.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way\n",
    "with open('/home/mahfuz/Desktop/dlpyt/p1ch3/outpoint1.t', 'wb') as f:\n",
    "    torch.save(points, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our points back is similarly o one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.load('/home/mahfuz/Desktop/dlpyt/p1ch3/outpoints.t')\n",
    "\n",
    "# alternative way\n",
    "with open('/home/mahfuz/Desktop/dlpyt/p1ch3/outpoint1.t', 'rb') as f:\n",
    "    point1 = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### erializing to HDF5 with h5py\n",
    "We can install h5py using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41402c8b5e7f8806df4be0126c60543c8d738b82ccbc17a04411a376311c906e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
